{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizado los crímenes en la Ciudad de Nueva York\n",
    "\n",
    "__Integrantes:__ \n",
    "- Daniel Flores\n",
    "- Francisco Fernandez\n",
    "\n",
    "Sección G1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hito 1\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "El objetivo del problema es generar dos modelos predictivos relacionados con los stop-and-frisk (detenciones y cateos) realizadas por la policía de NYC (NYPD).\n",
    "\n",
    "El primer modelo debe predecir si un determinado procedimiento terminará en arresto o no en base a las características medidas del sospechoso: género, raza y que se arrestado en uno de los cincos barrios.\n",
    "\n",
    "El segundo modelo debe predecir si el procedimiento finalizará en una acción violenta.\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Cada vez que un policía detiene a una persona en NYC, el oficial debe completar un formulario registrando los detalles de la detención. Los formularios eran llenados a mano hasta el año 2017, cuando los formularios en papel se conviertieron a electrónicos. La policía informa las detenciones y cateos de dos maneras: un informe resumido publicado trimestralmente y una base de datos completa publicada anualmente.\n",
    "\n",
    "Los informes trimestrasles son publicados cada tres meses, incluyen información de detenciones, arrestos y citaciones. Los datos son desglosados por distrito policial, raza y genero de la persona detenida.\n",
    "\n",
    "La base de datos anual incluye practicamente toda la información registrada por el oficial de policía después de una detención, tal como la edad, si la persona fue cateada, si tenía un arma o si se recuperó un arma de fuego, si se usó fuerza física en la detención, y la ubicación exacta de la detenciín dentro del distrito policial. La NYPD sube esta base de datos a su página web anualmente. La base de datos contiene más de 100 variables y dependiendo del año, sobre 10.000 observaciones, cada registro representa una detención realizada por un oficial de policía.\n",
    "\n",
    "**Controversia**\n",
    "\n",
    "Los procedimientos stop-and-frisk han sido muy criticado debido su alto porcentaje de detenciones de personas inocentes --sobre 80% entre 2002 y 2015-- y a que la mayoría de las detenciones recaen en personas de raza negra y latinos, lo mismo sucede con el uso de la fuerza.\n",
    "\n",
    "Resumen de datos del año 2009:\n",
    "- 581.168 procedimientos registrados.\n",
    "- 510.742 eran inocentes (88%).\n",
    "- 310.611 eran de raza negra (55%).\n",
    "- 180.055 eran latinos (32%).\n",
    "- 53.601 eran blancos (10%).\n",
    "- 289,602 tenían entre 14-24 años (50%).\n",
    "\n",
    "Desde el año 2011 hubo una reducción constante de los procedimientos registrados, disminuyendo el porcentaje de inocentes detenidos, pero se mantuvo la parcialidad hacia las razas negras y latinas:\n",
    "\n",
    "Resumen de dartos del año 2018:\n",
    "- 11.008 procedimientos registrados. \n",
    "- 7.645 eran inocentes (70%).\n",
    "- 6,241 eran de raza negra (57%).\n",
    "- 3,389 eran latinos (31%).\n",
    "- 1,074 eran blancos (10%).\n",
    "\n",
    "Fuente: según [STOP-AND-FRISK DATA](https://www.nyclu.org/en/stop-and-frisk-data)\n",
    "\n",
    "**Se debe tener en cuenta, a priori, que existirá una parcialidad racial en la data de entrenamiento y posterior evaluación**.\n",
    "\n",
    "### Desarrollo de la solución\n",
    "\n",
    "Se seguirá el flujo habitual de machine learning:\n",
    "\n",
    "1. **Importación/obtención de datos.**\n",
    "2. **Análisis de datos:** Se analizarán los datos pérdidos y outliers, se generarán tendencias y estadística descriptiva de los atributos y vectores objetivos. Se presentará el esquema de recodificación en caso de requerirse.\n",
    "3. **Preprocesamiento:** Se transformarán (recodificarán) y limpiarán datos. Se normalizarán los atributos. \n",
    "4. **Búsqueda y selección de algoritmos a ocupar en base a objetivos del modelo.** En esta sección en base al conocimiento previo y a búsqueda de problemas similares se seleccionarán los algoritmos a utilizar para los modelos. Selección de métricas para modelos.\n",
    "5. **Entrenar y validación del modelo:** \n",
    "Se dividirán los datos en grupos: Entrenamiento, validación y **prueba PENDIENTE NOMBRE**. Se entrenarán y compararán los modelos utilizando los algoritmos seleccionados del item anterior en un GridSearch con la métrica previamente seleccionada. Los modelos tendrán hyperparámetros básicos.\n",
    "Se seleccionará el modelo con mejor métrica y se ajustarán sus hyperparámetros utilizando los datos de validación.\n",
    "Podría ser necesario recodificar datos.\n",
    "6. **Evaluación del modelo**\n",
    "Finalmente se evaluará el modelo seleccionado para cada problema, obteniéndo sus métricas, y finalmente se serializará."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspectos computacionales\n",
    "\n",
    "Se utilizara las principales librerias para el analisis de datos, las cuales son:\n",
    "\n",
    "### Bibliotecas de Python \n",
    " \n",
    "- `pandas`:  Permite realizar tareas de manipulación, agregación y visualización de datos de forma más sencilla.\n",
    "- `numpy`: Agrega soporte para vectores y matrices y  funciones matemáticas de alto nivel para operar con esos datos. \n",
    "- `matplotlib`: Permite crear visualizaciones de datos simples.\n",
    "- `seaborn`: Permite visualizar modelos estadísticos. Se basa en Matplotlib.\n",
    "- `statsmodels`: Estimación de modelos estadísticos, en nuestro caso la regresion logística.\n",
    "- `scikit-learn`: Implementa aprendizaje de máquina, incluye varios algoritmos de clasificación, regresión y métricas, incluyendo varias herramientas útiles, como separación de datos de entrenamiento, validación cruzada y codificación de variables categóricas.\n",
    "- `factor_analizer`: Permite implementar el EFA (análisis factorial exploratorio).\n",
    "- `missingno`: Biblioteca para la visualización de datos perdidos.\n",
    "- `warnings`: Evitará que aparezcan las advertencias de avisos de deprecación.\n",
    "- `IPython.display`: Embellece el output de salida. \n",
    "\n",
    "\n",
    "### Módulos y funciones de Python\n",
    "\n",
    "- `utils`: Módulo que contiene función para graficar\n",
    "- `preproc_nyc_sqf`: Modulo con funciones básicas de limpieza de datos faltantes, transformación de etiquetas\n",
    "nulas en variables categóricas y crea atributos sinteticos de edad del sospechoso y conversión de distancia a sistema metrico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (preproc_nyc_sqf.py, line 29)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/danielf/Documents/Data Science/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3296\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-74d75c6f4220>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from preproc_nyc_sqf import create_suitable_dataframe\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/danielf/Documents/Data Science/3 - Machine Learning/Prueba/Prueba2/preproc_nyc_sqf.py\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    tmp_list = list(filter(lambda x: if x is not None, tmp_list))\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "import missingno as msngo\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from utils import get_graph\n",
    "from preproc_nyc_sqf import create_suitable_dataframe\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 112)\n",
    "pd.set_option(\"display.max_info_columns\", 112)\n",
    "pd.set_option(\"display.max_colwidth\", 3000)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de datos\n",
    "\n",
    "### Base de datos y procesamiento inicial\n",
    " \n",
    "La base de datos es del año 2009, contiene sólo el 1% de los datos, la base de datos original tiene 111 columnas y 5812 observaciones. \n",
    "\n",
    "Se realiza un procesamiento inicial de las variables que consiste en:\n",
    "- Eliminar valores nulos con dropna().\n",
    "- Eliminar las columnas con un único valor, por ejemplo se eliminó la columna año.\n",
    "- Eliminar las columnas categóricas con muchos valores únicos, por ejemplo se eliminó el número de calle.\n",
    "- Se cofificaron las columnas officrid, offshld, sector, trhsloc, beat y offverb. Por lo tanto, la nueva descripción de las columnas es:\n",
    "    - \"officrid\": ID CARD PROVIDED BY OFFICER? (Y: IF NOT IN UNIFORM, N)\n",
    "    - \"offshld\": SHIELD PROVIDED BY OFFICER? (Y: IF NOT IN UNIFORM, N)\n",
    "    - \"sector\": LOCATION OF STOP SECTOR (U FOR UNKNOWN)\n",
    "    - \"trhsloc\": WAS LOCATION HOUSING OR TRANSIT AUTHORITY ? (U FOR UNKNOWN)\n",
    "    - \"beat\": LOCATION OF STOP BEAT (U for UNKNOWN)\n",
    "    - \"offverb\": VERBAL STATEMENT PROVIDED BY OFFICER (IF NOT IN UNIFORM)?\n",
    "- Se agrega columna \"meters\" con altura del sospechoso en metros.\n",
    "- Se agrega columna de \"month\" con el mes en que ocurrió el procedimiento.\n",
    "- Se crea columna \"age_individual\" con la edad del sospechoso.\n",
    "\n",
    "Se reducen las columnas a 75 y 4636 observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importa base de datos original y aplica procesamiento inicial\n",
    "\n",
    "df = pd.read_csv(\"2009_1perc.csv\", index_col=0)\n",
    "display(Markdown(\"**Base de datos original**: Tiene {} columnas y {} \".format(df.shape[1], df.shape[0])))\n",
    "\n",
    "df, _, _ = create_suitable_dataframe(df)\n",
    "display(Markdown(\"**Base de datos procesada**: Tiene {} columnas y {} \".format(df.shape[1], df.shape[0])))\n",
    "\n",
    "# Importa descripción de variables\n",
    "df_spec = pd.read_csv(\"2009 SQF File Spec.csv\", sep=\";\")\n",
    "df_spec.set_index(\"Variable\", inplace=True)\n",
    "\n",
    "# Mantiene solo columnas en el nuevo dataframe y modifica/agrega descripciones\n",
    "df_desc = df_spec[df_spec.index.isin(df.columns)]\n",
    "\n",
    "df_desc.loc[\"officrid\",[\"Label\"]] = \"ID CARD PROVIDED BY OFFICER? (Y: IF NOT IN UNIFORM, N)\"\n",
    "df_desc.loc[\"offshld\",[\"Label\"]] = \"SHIELD PROVIDED BY OFFICER? (Y: IF NOT IN UNIFORM, N)\"\n",
    "df_desc.loc[\"sector\",[\"Label\"]] = \"LOCATION OF STOP SECTOR (U FOR UNKNOWN)\"\n",
    "df_desc.loc[\"trhsloc\",[\"Label\"]] = \"WAS LOCATION HOUSING OR TRANSIT AUTHORITY ? (U FOR UNKNOWN)\"\n",
    "df_desc.loc[\"beat\",[\"Label\"]] = \"LOCATION OF STOP BEAT (U for UNKNOWN)\"\n",
    "df_desc.loc[\"offverb\",[\"Label\"]] = \"VERBAL STATEMENT PROVIDED BY OFFICER (IF NOT IN UNIFORM)?\"\n",
    "df_desc.loc[\"meters\",[\"Label\"]] = \"SUSPECT'S HEIGHT\"\n",
    "df_desc.loc[\"month\",[\"Label\"]] = \"MONTH\"\n",
    "df_desc.loc[\"age_individual\",[\"Label\"]] = \"SUSPECT'S AGE\"\n",
    "\n",
    "\n",
    "df_desc.loc[:, [\"Label\"]].head(df_desc.shape[0]).sort_index(axis=\"index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis y visualización de datos pérdidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente gráfico de barras muestra como se ven afectadas las variables al considerar los valores \"U\" (UNKNOWN) como pérdidos, y valores con espacios en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unknown = df.replace(to_replace=\"U\", value=np.nan)\n",
    "df_unknown = df_unknown.replace(to_replace=\" \", value=np.nan)\n",
    "\n",
    "filtered_data = msngo.nullity_filter(\n",
    "    df_unknown, filter='bottom', n=15, p=0.999\n",
    ")\n",
    "\n",
    "color = (0.171, 0.637, 0.328)\n",
    "#color = (0.629, 0.848, 0.606)\n",
    "\n",
    "plt.figure()\n",
    "msngo.matrix(filtered_data, color=color)\n",
    "\n",
    "plt.figure()\n",
    "msngo.bar(filtered_data, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las columnas \"beat\" y \"post\" tienen muchos valores desconocidos, sobre 60% y 90% respectivamente. Por lo que se eliminarán ambas columnas, la información de ambas no es indispensable ya que hay otras variables que indican posición. \n",
    "\n",
    "El resto de valores pérdidos son bajos con respecto al total de datos (<5% por columna, sólo 4 variables). Por lo que se eliminarán las filas completas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unknown.drop(axis=\"columns\", columns=[\"beat\", \"post\"], inplace=True)\n",
    "df = df_unknown.dropna()\n",
    "display(Markdown(\"**Base de datos procesada**: Tiene {} columnas y {} \".format(df.shape[1], df.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis y visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_graph(df.sort_index(axis=\"index\"))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
